{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-pOkgUW0mgn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmtiqBGw0VVO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFkKKOe31yyz"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEqR9av5dsh"
      },
      "source": [
        "Defining the Taxi-v3 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBQWw4bw1yKR"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIyN49T66tn_"
      },
      "source": [
        "Since the observation is an integer encoding information about the state, we need a function to decode the information as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrGoPvSE6ERK"
      },
      "outputs": [],
      "source": [
        "def decodeObservation(state):\n",
        "  position = state//20\n",
        "  col = position%5\n",
        "  row = position//5\n",
        "  pick_drop_information = state%20\n",
        "  pick_location = pick_drop_information//4\n",
        "  drop_location = pick_drop_information%4\n",
        "  return row, col, pick_location, drop_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21wYchOg3gI3"
      },
      "outputs": [],
      "source": [
        "taxi_row = 4\n",
        "taxi_col = 2\n",
        "passenger_location = 4\n",
        "destination = 3\n",
        "decodeObservation(((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iquRx5B14Xm"
      },
      "source": [
        "## Epsilon-Greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS9jwrkB1_WA"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if q_values.any() and np.random.rand() > epsilon:\n",
        "    action = np.argmax(q_values)\n",
        "  else:\n",
        "    action = np.random.choice(len(q_values))\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmTmpacz11yN"
      },
      "source": [
        "## Options and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRlv6Q9R2xJn"
      },
      "source": [
        "We define four options here:\n",
        "\n",
        "\n",
        "*   Go to R (0)\n",
        "*   Go to G (1)\n",
        "*   Go to Y (2)\n",
        "*   Go to B (3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRPeUTH_7VQL"
      },
      "outputs": [],
      "source": [
        "colour_states_positions = [(0,0), (0,4), (4,0), (4,3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-AT3ibV1-8_"
      },
      "outputs": [],
      "source": [
        "def OptionGiven(state, q_table_option_policies, epsilon, goal_row, goal_column, option_number):\n",
        "\n",
        "  optdone = False\n",
        "  taxi_row, taxi_column, _, _ = decodeObservation(state)\n",
        "\n",
        "  if (taxi_row==goal_row and taxi_column==goal_column):\n",
        "    optdone = True\n",
        "\n",
        "  optact = epsilon_greedy(q_table_option_policies[option_number][state], epsilon)\n",
        "\n",
        "  return optdone, optact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvX8Xyxg17yl"
      },
      "source": [
        "## SMDP Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to run SMDP Q-Learning algorithm on the environment.\n",
        "\n"
      ],
      "metadata": {
        "id": "wCqv6Sb0Bi2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VncrwDqC5Qp7"
      },
      "outputs": [],
      "source": [
        "def SMDP_Q_Learning(env, gamma, alpha, epsilon_start, epsilon_end, epsilon_decay, num_episodes, Option, q_table_option_policies, q_table_SMDP, final_options, frames):\n",
        "  rewards = []\n",
        "  epsilon = epsilon_start\n",
        "  successes = 0\n",
        "  episode = 0\n",
        "  for _ in tqdm(range(num_episodes)):\n",
        "    episode+=1\n",
        "    epsilon = max(epsilon_end, epsilon*epsilon_decay)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "\n",
        "      action = epsilon_greedy(q_table_SMDP[state], epsilon)\n",
        "\n",
        "      if(episode==num_episodes-1):\n",
        "        final_options.append(action)\n",
        "\n",
        "      # checking if option chosen\n",
        "      reward_bar = 0\n",
        "      gamma_option = 1\n",
        "      curr_state = state\n",
        "\n",
        "      if action >= 0:\n",
        "\n",
        "        optdone = False\n",
        "        option_number = action\n",
        "        goal_row, goal_column = colour_states_positions[option_number]\n",
        "        while (optdone == False and done == False):\n",
        "\n",
        "          optdone,optact = Option(state, q_table_option_policies, epsilon, goal_row, goal_column, option_number)\n",
        "          next_state, reward, done, _ = env.step(optact)\n",
        "          if(episode==num_episodes):\n",
        "            frames.append({\n",
        "                'frame': env.render(mode='ansi'),\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'reward': reward\n",
        "                }\n",
        "            )\n",
        "          total_reward+= reward\n",
        "\n",
        "          if(reward == 20):\n",
        "            successes+=1\n",
        "\n",
        "          reward_bar = reward_bar + gamma_option*reward\n",
        "          gamma_option*=gamma\n",
        "          q_table_option_policies[option_number][state][optact] += alpha * (reward + gamma * np.max(q_table_option_policies[option_number][next_state]) - q_table_option_policies[option_number][state][optact])\n",
        "\n",
        "          state = next_state\n",
        "\n",
        "        q_table_SMDP[curr_state][action] += alpha * (reward_bar + gamma_option * np.max(q_table_SMDP[state]) - q_table_SMDP[curr_state][action])\n",
        "\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "  print(successes/num_episodes * 100)\n",
        "  return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below considers 5 train runs."
      ],
      "metadata": {
        "id": "JfhK9g_8BpVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgqfeD0cH_NY"
      },
      "outputs": [],
      "source": [
        "reward_vals = []\n",
        "q_tables_option_policies = []\n",
        "q_tables_SMDP = []\n",
        "for i in range(5):\n",
        "  final_options = []\n",
        "  frames = []\n",
        "  q_table_option_policies = np.zeros((4, 500, 6)) # num_options x num_states x num_primitive_actions_for_moving\n",
        "  q_table_SMDP = np.zeros((500, 4)) # num_states x num_options\n",
        "  rewards = SMDP_Q_Learning(env, 0.9, 0.1, 1, 0.0001, 0.99, 10000, OptionGiven, q_table_option_policies, q_table_SMDP, final_options, frames)\n",
        "  q_tables_SMDP.append(q_table_SMDP)\n",
        "  q_tables_option_policies.append(q_table_option_policies)\n",
        "  reward_vals.append(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmaps of Q Tables"
      ],
      "metadata": {
        "id": "z4NAqAAFB6eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averaging the q tables for further inference."
      ],
      "metadata": {
        "id": "siK94ticBzFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_tables_option_policies = np.array(q_tables_option_policies)\n",
        "q_tables_SMDP = np.array(q_tables_SMDP)\n",
        "q_table_option_policies_avgd = np.mean(q_tables_option_policies, axis=0)\n",
        "q_table_SMDP_avgd = np.mean(q_tables_SMDP, axis=0)"
      ],
      "metadata": {
        "id": "H7vMDDcU2MCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the pickup and drop phase option policy heatmaps."
      ],
      "metadata": {
        "id": "eV_kVjbvB9h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_loc = 2\n",
        "drop_location = 3\n",
        "\n",
        "heatmap_pick = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == passenger_loc and drop == drop_location:\n",
        "    heatmap_pick[row,col] = np.argmax(q_table_option_policies_avgd[drop_location][state])\n",
        "\n",
        "\n",
        "plt.imshow(heatmap_pick, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'south',1:'north', 2:'east', 3:'west', 4:'pickup', 5:'drop'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_pick[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "heatmap_drop = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == 4 and drop == drop_location:\n",
        "    heatmap_drop[row,col] = np.argmax(q_table_option_policies_avgd[drop_location][state])\n",
        "\n",
        "\n",
        "plt.imshow(heatmap_drop, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'south',1:'north', 2:'east', 3:'west', 4:'pickup', 5:'drop'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_drop[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RA4cfY40CWvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_loc = 3 # B\n",
        "drop_location = 2 # Y\n",
        "\n",
        "heatmap = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == passenger_loc and drop == drop_location:\n",
        "    heatmap[row,col] = np.argmax(q_table_SMDP_avgd[state])\n",
        "\n",
        "\n",
        "plt.imshow(heatmap, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'gotoR',1:'gotoG', 2:'gotoY', 3:'gotoB'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S1xhfekpwzoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Plots"
      ],
      "metadata": {
        "id": "d0Del5GUCWtK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIJTBRL18qNJ"
      },
      "outputs": [],
      "source": [
        "avg_rewards = []\n",
        "for i in range(10000):\n",
        "  sum = 0\n",
        "  for j in range(5):\n",
        "    sum += reward_vals[j][i]\n",
        "  avg_rewards.append(sum/5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qj037DZ0zT-"
      },
      "outputs": [],
      "source": [
        "avg_rews = [np.average(avg_rewards[i:i+100]) for i in range(len(avg_rewards)-100)]\n",
        "x = np.arange(9900)\n",
        "t = [5] * len(avg_rews)\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.plot(x, np.array(avg_rews))\n",
        "plt.plot(x, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visulaizing agent's action"
      ],
      "metadata": {
        "id": "uAVaI-HMChCi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ortNIp3n5vj"
      },
      "outputs": [],
      "source": [
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.7)\n",
        "\n",
        "print_frames(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNWQgMvA0Czv"
      },
      "source": [
        "## Intra Option Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to run Intra Option Q-Learning algorithm on the environment.\n"
      ],
      "metadata": {
        "id": "PLqLpTzIDRn0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLsafNI7_FNp"
      },
      "outputs": [],
      "source": [
        "def IntraOption_Q_Learning(env, gamma, alpha, epsilon_start, epsilon_end, epsilon_decay, num_episodes, Option, q_table_option_policies, q_table_io, final_options, frames):\n",
        "  rewards = []\n",
        "  epsilon = epsilon_start\n",
        "  successes = 0\n",
        "  episode = 0\n",
        "  for _ in tqdm(range(num_episodes)):\n",
        "    episode+=1\n",
        "    epsilon = max(epsilon_end, epsilon*epsilon_decay)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "\n",
        "      action = epsilon_greedy(q_table_SMDP[state], epsilon)\n",
        "\n",
        "      if(episode==num_episodes-1):\n",
        "        final_options.append(action)\n",
        "\n",
        "      reward_bar = 0\n",
        "      gamma_option = 1\n",
        "      curr_state = state\n",
        "\n",
        "      if action >= 0:\n",
        "\n",
        "        optdone = False\n",
        "        option_number = action\n",
        "        goal_row, goal_column = colour_states_positions[option_number]\n",
        "        while (optdone == False and done == False):\n",
        "\n",
        "          optdone,optact = Option(state, q_table_option_policies, epsilon, goal_row, goal_column, option_number)\n",
        "          next_state, reward, done, _ = env.step(optact)\n",
        "          if(episode==num_episodes):\n",
        "            frames.append({\n",
        "                'frame': env.render(mode='ansi'),\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'reward': reward\n",
        "                }\n",
        "            )\n",
        "          total_reward+= reward\n",
        "\n",
        "          if(reward == 20):\n",
        "            successes+=1\n",
        "\n",
        "          reward_bar = reward_bar + gamma_option*reward\n",
        "          gamma_option*=gamma\n",
        "          q_table_option_policies[option_number][state][optact] += alpha * (reward + gamma * np.max(q_table_option_policies[option_number][next_state]) - q_table_option_policies[option_number][state][optact])\n",
        "          for opt_num in range(4):\n",
        "            q_table_io[state][opt_num] += alpha*(reward+gamma*((1-optdone)*q_table_io[next_state][opt_num]+optdone*np.max(q_table_io[next_state]))-q_table_io[state][opt_num])\n",
        "\n",
        "          state = next_state\n",
        "\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "  print(successes/num_episodes * 100)\n",
        "  return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell consdiers 5 runs of the algorithm."
      ],
      "metadata": {
        "id": "XRm1kVbhCzLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elfDCRIG_c5Q"
      },
      "outputs": [],
      "source": [
        "reward_vals = []\n",
        "q_tables_option_policies = []\n",
        "q_tables_io = []\n",
        "for i in range(5):\n",
        "  final_options = []\n",
        "  frames = []\n",
        "  q_table_option_policies = np.zeros((4, 500, 6)) # num_options x num_states x num_primitive_actions_for_moving\n",
        "  q_table_io = np.zeros((500, 4)) # num_states x num_options\n",
        "  rewards = IntraOption_Q_Learning(env, 0.9, 0.1, 1, 0.001, 0.99, 10000, OptionGiven, q_table_option_policies, q_table_io, final_options, frames)\n",
        "  q_tables_io.append(q_table_io)\n",
        "  q_tables_option_policies.append(q_table_option_policies)\n",
        "  reward_vals.append(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmaps"
      ],
      "metadata": {
        "id": "uWsCFlNpC23r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averaging q tables for further inference."
      ],
      "metadata": {
        "id": "6uGnPNeCC4aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_tables_option_policies = np.array(q_tables_option_policies)\n",
        "q_tables_io = np.array(q_tables_io)\n",
        "q_table_option_policies_avgd = np.mean(q_tables_option_policies, axis=0)\n",
        "q_table_io_avgd = np.mean(q_tables_io, axis=0)"
      ],
      "metadata": {
        "id": "-iw2_A0PIf7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting option policy q table heatmap"
      ],
      "metadata": {
        "id": "W_T6loQHC9iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_loc = 0\n",
        "drop_location = 3\n",
        "\n",
        "heatmap_pick = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == passenger_loc and drop == drop_location:\n",
        "    heatmap_pick[row,col] = np.argmax(q_table_option_policies_avgd[drop_location][state])\n",
        "\n",
        "\n",
        "plt.imshow(heatmap_pick, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'south',1:'north', 2:'east', 3:'west', 4:'pickup', 5:'drop'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_pick[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "heatmap_drop = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == 4 and drop == drop_location:\n",
        "    heatmap_drop[row,col] = np.argmax(q_table_option_policies_avgd[drop_location][state])\n",
        "\n",
        "\n",
        "plt.imshow(heatmap_drop, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'south',1:'north', 2:'east', 3:'west', 4:'pickup', 5:'drop'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_drop[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6O4p-JGdIiPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting heatmap for Intra-Option q table"
      ],
      "metadata": {
        "id": "SXNJhS6hDC2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_loc = 1 # Y\n",
        "drop_location = 3 # B\n",
        "\n",
        "heatmap_pick = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == passenger_loc and drop == drop_location:\n",
        "    heatmap_pick[row,col] = np.argmax(q_table_io_avgd[state])\n",
        "\n",
        "\n",
        "plt.imshow(heatmap_pick, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'gotoR',1:'gotoG', 2:'gotoY', 3:'gotoB'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_pick[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JOpPyX1czoi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Plot"
      ],
      "metadata": {
        "id": "p7CuRHVTDHse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUXuI7XTAfOu"
      },
      "outputs": [],
      "source": [
        "avg_rewards = []\n",
        "for i in range(10000):\n",
        "  sum = 0\n",
        "  for j in range(5):\n",
        "    sum += reward_vals[j][i]\n",
        "  avg_rewards.append(sum/5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7v6jLGi1ad8"
      },
      "outputs": [],
      "source": [
        "avg_rews2 = [np.average(avg_rewards[i:i+100]) for i in range(len(avg_rewards)-100)]\n",
        "x = np.arange(9900)\n",
        "t = [9.7] * len(avg_rews2)\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.plot(x, np.array(avg_rews2))\n",
        "plt.plot(x, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing agent's action"
      ],
      "metadata": {
        "id": "t8SXIWZuDbwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix6pAmkG1Ra7"
      },
      "outputs": [],
      "source": [
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.7)\n",
        "\n",
        "print_frames(frames)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}