{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "z-pOkgUW0mgn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NmtiqBGw0VVO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "RFkKKOe31yyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Taxi-v3 environment"
      ],
      "metadata": {
        "id": "AEEqR9av5dsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBQWw4bw1yKR",
        "outputId": "6358a06d-59f7-4657-c282-28e914b9c933"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the observation is an integer encoding information about the state, we need a function to decode the information as below:"
      ],
      "metadata": {
        "id": "bIyN49T66tn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decodeObservation(state):\n",
        "  position = state//20\n",
        "  col = position%5\n",
        "  row = position//5\n",
        "  pick_drop_information = state%20\n",
        "  pick_location = pick_drop_information//4\n",
        "  drop_location = pick_drop_information%4\n",
        "  return row, col, pick_location, drop_location"
      ],
      "metadata": {
        "id": "mrGoPvSE6ERK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epsilon-Greedy"
      ],
      "metadata": {
        "id": "8iquRx5B14Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if q_values.any() and np.random.rand() > epsilon:\n",
        "    action = np.argmax(q_values)\n",
        "  else:\n",
        "    action = np.random.choice(len(q_values))\n",
        "  return action"
      ],
      "metadata": {
        "id": "LS9jwrkB1_WA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colour_states_positions = [(0,0), (0,4), (4,0), (4,3)]"
      ],
      "metadata": {
        "id": "xRPeUTH_7VQL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option Function:\n",
        "we define an alternate option function different to the previous option set"
      ],
      "metadata": {
        "id": "LiFPKYn5HsHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OptionAlternate(state, q_table_option_policies, epsilon, goal_row, goal_column, option_number):\n",
        "\n",
        "  optdone = False\n",
        "  optact = epsilon_greedy(q_table_option_policies[option_number][state], epsilon)\n",
        "  next_state, reward, done, _ = env.step(optact)\n",
        "  taxi_row, taxi_col, pick, drop = decodeObservation(next_state)\n",
        "\n",
        "  if (pick < 4 and option_number == 0 and (taxi_row, taxi_col) == colour_states_positions[pick]):#check if the option is completed\n",
        "    optdone = True\n",
        "\n",
        "  elif (pick == 4 and option_number == 1 and (taxi_row, taxi_col) == colour_states_positions[drop]):#check if the option is completed\n",
        "    optdone = True\n",
        "\n",
        "  return optdone,optact,next_state,reward,done"
      ],
      "metadata": {
        "id": "Jzos0uzrNgZu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SMDP_Q_Learning(env, gamma, alpha, epsilon_start, epsilon_end, epsilon_decay, num_episodes, Option, q_table_option_policies, q_table_SMDP, final_options, frames):\n",
        "  rewards = []\n",
        "  epsilon = epsilon_start\n",
        "  successes = 0\n",
        "  episode = 0\n",
        "  for _ in tqdm(range(num_episodes)):\n",
        "    episode+=1\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "\n",
        "      action = epsilon_greedy(q_table_SMDP[state], epsilon)\n",
        "      epsilon = max(epsilon_end, epsilon*epsilon_decay)\n",
        "      if(episode==num_episodes):\n",
        "        final_options.append(action)\n",
        "      curr_state = state\n",
        "      #if the option is drop or fetch\n",
        "      if action>1:\n",
        "        next_state, reward, done,terminated= env.step(action+2)\n",
        "        q_table_SMDP[curr_state][action] += alpha * (reward + gamma* np.max(q_table_SMDP[next_state]) - q_table_SMDP[curr_state][action])\n",
        "        state=next_state\n",
        "        total_reward+=reward\n",
        "      if(episode==num_episodes):#for rendering purposes\n",
        "        frames.append({\n",
        "          'frame': env.render(mode='ansi'),\n",
        "          'state': state,\n",
        "          'action': action,\n",
        "          'reward': reward\n",
        "          }\n",
        "          )\n",
        "      #if the option is gotodrop or gotofetch\n",
        "      if action <= 1:\n",
        "        reward_bar = 0\n",
        "        gamma_option = 1\n",
        "        optdone = False\n",
        "        option_number = action\n",
        "        goal_row, goal_column = colour_states_positions[option_number]\n",
        "        while (optdone == False and done == False):\n",
        "\n",
        "          optdone,optact,next_state,reward,done= OptionAlternate(state, q_table_option_policies, epsilon, goal_row, goal_column, option_number)\n",
        "          if(episode==num_episodes):\n",
        "            frames.append({\n",
        "                'frame': env.render(mode='ansi'),\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'reward': reward\n",
        "                }\n",
        "            )\n",
        "          total_reward+= reward\n",
        "          if(reward == 20):\n",
        "            successes+=1\n",
        "          reward_bar = reward_bar + gamma_option*reward\n",
        "          gamma_option*=gamma\n",
        "          q_table_option_policies[option_number][state][optact] += alpha * (reward + gamma * np.max(q_table_option_policies[option_number][next_state]) - q_table_option_policies[option_number][state][optact])\n",
        "          state = next_state\n",
        "        # Complete SMDP Q-Learning Update\n",
        "        q_table_SMDP[curr_state][action] += alpha * (reward_bar + gamma_option * np.max(q_table_SMDP[state]) - q_table_SMDP[curr_state][action])\n",
        "    rewards.append(total_reward)\n",
        "  print(successes/num_episodes * 100)\n",
        "  return rewards"
      ],
      "metadata": {
        "id": "3KWeKYwdNo3V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Averaging over 5 runs:"
      ],
      "metadata": {
        "id": "6Xas-mo4ItPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_vals = []\n",
        "q_tables_option_policies=[]\n",
        "q_tables_SMDP=[]\n",
        "for i in range(5):#averaging over 5 runs\n",
        "  final_options = []\n",
        "  frames = []\n",
        "  q_table_option_policies = np.zeros((2, 500, 4)) # num_options x num_states x num_primitive_actions_for_moving\n",
        "  q_table_SMDP = np.zeros((500, 4)) # num_states x num_options\n",
        "  rewards = SMDP_Q_Learning(env, 0.9, 0.1, 1, 0.0001, 0.99, 10000, OptionAlternate, q_table_option_policies, q_table_SMDP, final_options, frames)\n",
        "  reward_vals.append(rewards)\n",
        "  q_tables_option_policies.append(q_table_option_policies)\n",
        "  q_tables_SMDP.append(q_table_SMDP)\n",
        "q_table_option_policies_avg=np.mean(q_tables_option_policies,axis=0)\n",
        "q_table_SMDP_avg=np.mean(q_tables_SMDP,axis=0)"
      ],
      "metadata": {
        "id": "F1rvGIvjNs3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For plotting:"
      ],
      "metadata": {
        "id": "ysXqAjf5I3Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_rewards = []\n",
        "for i in range(10000):\n",
        "  sum = 0\n",
        "  for j in range(5):\n",
        "    sum += reward_vals[j][i]\n",
        "  avg_rewards.append(sum/5)"
      ],
      "metadata": {
        "id": "06Lsn11uRFfV"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(10000)\n",
        "t = [10] * 10000\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.plot(x, np.array(avg_rewards))\n",
        "plt.plot(x, t)"
      ],
      "metadata": {
        "id": "96QzrOm6RJbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For animation:"
      ],
      "metadata": {
        "id": "uWzEz9slI7BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.7)\n",
        "\n",
        "print_frames(frames)"
      ],
      "metadata": {
        "id": "Q3eR1jAmRSe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Average plot:"
      ],
      "metadata": {
        "id": "Lw6CIKGRI-y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_rews2 = [np.average(avg_rewards[i:i+100]) for i in range(len(avg_rewards)-100)]\n",
        "x = np.arange(9900)\n",
        "t = [8.5] * len(avg_rews2)\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.plot(x, np.array(avg_rews2))\n",
        "plt.plot(x, t)\n"
      ],
      "metadata": {
        "id": "6waA07EPU1gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For plotting option policy Heatmap"
      ],
      "metadata": {
        "id": "1thV2FN9JLJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_loc = 0\n",
        "drop_location = 2\n",
        "map={(0,0):0,(0,4):1,(4,0):2,(4,3):3}\n",
        "heatmap_pick = np.zeros((5,5))# gridsize\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == passenger_loc: #and drop == drop_location:\n",
        "    heatmap_pick[row,col] = np.argmax(q_table_option_policies_avg[0][state])\n",
        "    if ((row,col) in map and map[(row,col)]==passenger_loc):\n",
        "      heatmap_pick[row,col]=np.argmax(q_table_SMDP_avg[state])+2\n",
        "\n",
        "plt.imshow(heatmap_pick, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'south',1:'north', 2:'east', 3:'west', 4:'pickup', 5:'drop'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_pick[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "heatmap_drop = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == 4 and drop == drop_location:\n",
        "    heatmap_drop[row,col] = np.argmax(q_table_option_policies_avg[1][state])\n",
        "    if ((row,col) in map and map[(row,col)]==drop_location):\n",
        "      heatmap_drop[row,col]=np.argmax(q_table_SMDP_avg[state])+2\n",
        "\n",
        "plt.imshow(heatmap_drop, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'south',1:'north', 2:'east', 3:'west', 4:'pickup', 5:'drop'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_drop[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1cgOZs3pBhzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For plotting overall policy heatmap:"
      ],
      "metadata": {
        "id": "3j1hBH84JZhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_loc = 0\n",
        "drop_location = 3\n",
        "map={(0,0):0,(0,4):1,(4,0):2,(4,3):3}\n",
        "heatmap_pick = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == passenger_loc: #and drop == drop_location:\n",
        "    heatmap_pick[row,col] = np.argmax(q_table_SMDP_avg[state])\n",
        "\n",
        "plt.imshow(heatmap_pick, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:\"fetch\",1:\"dest\",2:\"pick\",3:\"drop\"}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_pick[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "heatmap_drop = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == 4 and drop == drop_location:\n",
        "    heatmap_drop[row,col] = np.argmax(q_table_SMDP_avg[state])\n",
        "\n",
        "plt.imshow(heatmap_drop, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:\"fetch\",1:\"dest\",2:\"pick\",3:\"drop\"}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_drop[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "72g4x94d4bhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intra Option Q learning:"
      ],
      "metadata": {
        "id": "IGf9lMzcJvB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option function for intra q:"
      ],
      "metadata": {
        "id": "9dE8FfBPKB8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OptionAlternate(state, q_table_option_policies, epsilon, option_number,optdone):\n",
        "          optact=epsilon_greedy(q_table_option_policies[option_number][state],epsilon)\n",
        "          next_state,reward,done,terminated=env.step(optact)\n",
        "          taxi_row, taxi_col, pick, drop = decodeObservation(next_state)\n",
        "          if pick!=4 and option_number==0 and (taxi_row,taxi_col)==colour_states_positions[pick]:\n",
        "            optdone=True\n",
        "          if pick==4 and option_number==1 and (taxi_row,taxi_col)==colour_states_positions[drop]:\n",
        "            optdone=True\n",
        "\n",
        "          return optdone,optact,next_state,reward,done,taxi_row,taxi_col,pick,drop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4GrZy2tdeYt",
        "outputId": "a05ad151-29ab-48f7-c166-34c1be9b780c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qlearning function"
      ],
      "metadata": {
        "id": "Vxi4P2vIKHvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IntraOption_Q_Learning(env, gamma, alpha, epsilon_start, epsilon_end, epsilon_decay, num_episodes, Option, q_table_option_policies, q_table_io, final_options, frames):\n",
        "  rewards = []\n",
        "  epsilon = epsilon_start\n",
        "  successes = 0\n",
        "  episode = 0\n",
        "  for _ in tqdm(range(num_episodes)):\n",
        "    episode+=1\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "      epsilon=max(epsilon_end,epsilon*epsilon_decay)\n",
        "      option=epsilon_greedy(q_table_io[state],epsilon)\n",
        "      #if option is pick or drop\n",
        "      if option>1:\n",
        "        next_state, reward, done,terminated= env.step(option+2)\n",
        "        total_reward+=reward\n",
        "        q_table_io[state][option]+=alpha*(reward + gamma*np.max(q_table_io[next_state])-q_table_io[state][option])\n",
        "        if(episode==num_episodes):#for rendering purposes\n",
        "            frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'state': state,\n",
        "            'action': option,\n",
        "            'reward': reward\n",
        "            }\n",
        "            )\n",
        "        state=next_state\n",
        "      #if option is gotopickup or gotodrop\n",
        "      if option<=1:\n",
        "        optdone=False\n",
        "        while(optdone==False):\n",
        "          optdone,optact,next_state,reward,done,taxi_row,taxi_col,pick,drop=OptionAlternate(state, q_table_option_policies, epsilon, option,optdone)\n",
        "          if(episode==num_episodes):#for rendering purposes\n",
        "            frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'state': state,\n",
        "            'action': option,\n",
        "            'reward': reward\n",
        "            }\n",
        "            )\n",
        "          total_reward+=reward\n",
        "          #IntraQ update rules\n",
        "          for i in range(2):\n",
        "            if (pick!=4 and i==0 and (taxi_row,taxi_col)==colour_states_positions[pick]) or (pick==4 and i==1 and (taxi_row,taxi_col)==colour_states_positions[drop]):\n",
        "                q_table_io[state][i]+=alpha*(reward+gamma*np.max(q_table_io[next_state])-q_table_io[state][i])\n",
        "                q_table_option_policies[option][state][optact]+=alpha*(reward+gamma*np.max(q_table_option_policies[option][next_state])-q_table_option_policies[option][state][optact])\n",
        "            else:\n",
        "                q_table_io[state][i]+=alpha*(reward+gamma*(q_table_io[next_state][i])-q_table_io[state][i])\n",
        "                q_table_option_policies[option][state][optact]+=alpha*(reward+gamma*np.max(q_table_option_policies[option][next_state])-q_table_option_policies[option][state][optact])\n",
        "\n",
        "          state=next_state\n",
        "          if done:\n",
        "            break\n",
        "    rewards.append(total_reward)\n",
        "  return rewards"
      ],
      "metadata": {
        "id": "WPNmne8idRqE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Averaging over 5 runs"
      ],
      "metadata": {
        "id": "KLyz4ixnMiKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_vals = []\n",
        "option_tables=[]\n",
        "ioql_tables=[]\n",
        "for i in range(5):\n",
        "  final_options = []\n",
        "  frames = []\n",
        "  q_table_option_policies = np.zeros((2, 500, 4)) # num_options x num_states x num_primitive_actions_for_moving\n",
        "  q_table_io = np.zeros((500, 4)) # num_states x num_options\n",
        "  rewards = IntraOption_Q_Learning(env, 0.9, 0.1, 0.1, 0.0001, 0.99, 10000, OptionAlternate, q_table_option_policies, q_table_io, final_options, frames)\n",
        "  option_tables.append(q_table_option_policies)\n",
        "  ioql_tables.append(q_table_io)\n",
        "  reward_vals.append(rewards)\n",
        "option_avg=np.mean(option_tables,axis=0)\n",
        "ioql_avg=np.mean(ioql_tables,axis=0)"
      ],
      "metadata": {
        "id": "qKMRvFgRdzf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For plotting purposes:"
      ],
      "metadata": {
        "id": "g-pheDzYMg9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_rewards = []\n",
        "for i in range(10000):\n",
        "  sum = 0\n",
        "  for j in range(5):\n",
        "    sum += reward_vals[j][i]\n",
        "  avg_rewards.append(sum/5)"
      ],
      "metadata": {
        "id": "_GzU3R6qmSkv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(10000)\n",
        "t = [10] * 10000\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.plot(x, np.array(avg_rewards))\n",
        "plt.plot(x, t)"
      ],
      "metadata": {
        "id": "O-6Zf_Y1mV1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_rews2 = [np.average(avg_rewards[i:i+100]) for i in range(len(avg_rewards)-100)]\n",
        "x = np.arange(9900)\n",
        "t = [8.5] * len(avg_rews2)\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.plot(x, np.array(avg_rews2))\n",
        "plt.plot(x, t)"
      ],
      "metadata": {
        "id": "bzPf31QnmXLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Animation:"
      ],
      "metadata": {
        "id": "kHXpr1-AMo8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.7)\n",
        "\n",
        "print_frames(frames)"
      ],
      "metadata": {
        "id": "poHGXxY5MO5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option policy heatmap"
      ],
      "metadata": {
        "id": "T4_iusTjMt0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_loc = 2\n",
        "drop_location = 0\n",
        "map={(0,0):0,(0,4):1,(4,0):2,(4,3):3}\n",
        "heatmap_pick = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == passenger_loc: #and drop == drop_location:\n",
        "    heatmap_pick[row,col] = np.argmax(option_avg[0][state])\n",
        "    if ((row,col) in map and map[(row,col)]==passenger_loc):\n",
        "      heatmap_pick[row,col]=np.argmax(ioql_avg[state])+2\n",
        "\n",
        "plt.imshow(heatmap_pick, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'south',1:'north', 2:'east', 3:'west', 4:'pickup', 5:'drop'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_pick[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "heatmap_drop = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == 4 and drop == drop_location:\n",
        "    heatmap_drop[row,col] = np.argmax(option_avg[1][state])\n",
        "    if ((row,col) in map and map[(row,col)]==drop_location):\n",
        "      heatmap_drop[row,col]=np.argmax(ioql_avg[state])+2\n",
        "\n",
        "plt.imshow(heatmap_drop, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:'south',1:'north', 2:'east', 3:'west', 4:'pickup', 5:'drop'}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_drop[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W7zykxGSVwjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall policy heatmap"
      ],
      "metadata": {
        "id": "DCRf1aSTMxPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_loc = 0\n",
        "drop_location = 1\n",
        "map={(0,0):0,(0,4):1,(4,0):2,(4,3):3}\n",
        "heatmap_pick = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == passenger_loc: #and drop == drop_location:\n",
        "    heatmap_pick[row,col] = np.argmax(ioql_avg[state])\n",
        "\n",
        "\n",
        "plt.imshow(heatmap_pick, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:\"fetch\",1:\"dest\",2:\"pick\",3:\"drop\"}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_pick[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "heatmap_drop = np.zeros((5,5))\n",
        "for state in range(500):\n",
        "  row, col, pick, drop = decodeObservation(state)\n",
        "  if pick == 4 and drop == drop_location:\n",
        "    heatmap_drop[row,col] = np.argmax(ioql_avg[state])\n",
        "\n",
        "\n",
        "plt.imshow(heatmap_drop, cmap='viridis', interpolation='nearest')\n",
        "labels = {0:\"fetch\",1:\"dest\",2:\"pick\",3:\"drop\"}\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, labels[int(heatmap_drop[i, j])], ha='center', va='center', color='white')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CkojdAog17R3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}